#pragma once

#include "stdlibs.h"
#include "url.h"
#include "tag_parser.h"
#include "http_response.h"
#include "http_request.h"
#include "http_connection.h"
#include "crawler_viewer.h"

#if PLATFORM == PLATFORM_WINDOWS

#define SLEEP(ms) Sleep(ms)

#else

#define SLEEP(ms) usleep(ms * 1000)

#endif


namespace CrawlerImpl
{

    /**************************************************************************************************************
     *
     *   #1. Object of type Crawler saves in queue of internal links, only relative part of link.
     *   Likewise saves in indexed queue. The relative path URL must start with character slash "/".
     *
     **************************************************************************************************************
     *
     *   #2. Conversion functions relative addresses returns a new address, starting with the character slash "/".
     *
     **************************************************************************************************************
     *
     *   #3. Single slash - "/" in internal and indexed queues denotes the root of the site.
     *
     **************************************************************************************************************
     */


    class Crawler
    {
	private:
        // for determine internal/external/indexed queues
        // 0, 2, 4 for support transmission of packets
        // like this call as: function(FIRST_CONST | SECOND_CONST | THIRD_CONST);
        enum QueueType { Internal, External = 2, IndexedIn = 4, IndexedEx = 8 };

        // for addToQueue. This constants specifies which urls
        // should get from html page
        enum class UrlType { Internal, External, Default };

        std::deque<std::string> m_indexedInternal;
        std::deque<std::string> m_indexedExternal;
        std::deque<std::string> m_internalLinks;
        std::deque<std::string> m_externalLinks;

        // pointer because objects of type HttpResponse
        // may create and edit only objects of type HttpConnection
        const HttpLib::HttpResponse* m_response;
        HttpLib::HttpRequest m_request;
        HttpLib::HttpConnection m_controller;	

        HtmlParser::Url m_host; // main website URL to crawl
        std::size_t m_timePause; // between requests
        std::string m_signature; // full signature in User-agent
        std::string m_sign;	// original signature of crawler
        std::size_t m_maxDepth;

        bool m_readyForStart;
        bool m_infiniteCrawl;

        std::string m_pathToSave;

        const ViewHandler::CrawlerViewer *m_viewerDevice = nullptr;

        static const std::string s_storage_of_phrases;    // stores name of file which consists from words for generate phrases
        static const std::string s_report_url;            // name of file that stores errors generated by the class Apx::Url
        static const std::string s_storage_urls;          // name of file that stores the addresses of sites passed

        // =================================================================================

        // main cycle of index website
        void crawlStart();
        void crawlResource();

        // the second argument is expected
        // relative address indicating the file or without it
        void addToQueue(const HtmlParser::TagParser &parser, const HtmlParser::Url &relativePath, UrlType urls = UrlType::Default);

        // finds link in indexed and internal queues
        //bool existsInQueues(const std::string &lnk, int whereToSearch = INTERNAL);
        bool existsInQueues(const HtmlParser::Url &url, int whereToSearch = Internal);

        // add link into specified queue with check on uniqueness
        // if addition into internal queue then adds only relative path.
        // if addition into external queue then adds subdomain + host
        void addLinkToQueue(const HtmlParser::Url &url, int whereToSearch);

        // the flags you can pass through |. Old of C method.
        void clearQueue(int queueMark);

        // reset configurations with clear internal and indexedInternal queues
        void configByDefault();

        /** API for conversion true relative addresses **/

        // converts the relative path in accordance with the address where the received link
        // expects that second parameter is set by the website root
        std::string convertRelativeAddr(const HtmlParser::Url &relAddr, const HtmlParser::Url &where);

        // divides path by folders
        // expects path following [/]aaa/[bbb/[page.php[?var1=val&var2=val]]]
        // the last slash should be, otherwise will not correct the results
        std::vector<std::string> dividePath(const std::string &path);

    public:

        Crawler();
        Crawler(const std::string &startAddr);
        Crawler(const std::string &startAddr, std::size_t timePause);

        void setStartAddress(const std::string &startAddr);
		void setSignature(const std::string &signature);
		void setViewer(const ViewHandler::CrawlerViewer *viewer);
		void setPause(std::size_t timePause);
		void setMaxDepth(std::size_t depth = 0);
		void setModeInfiniteCrawl(bool val);
		void setPathToSavePages(const std::string &path);

        bool readyForStart();
        void start();

        void clearInternalQueue();
        void clearExternalQueue();
        void clearIndexedInQueue();
        void clearIndexedExQueue();

		std::size_t getTimePause() const;
		std::size_t getMaxDepth() const;
		bool getModeInfiniteCrawl() const; 
		const std::string& getSignature() const;
		const std::string& getStartAddress() const;
		const std::string& getPathToSavePages() const;
    };

}