#pragma once

#include "stdlibs.h"
#include "url.h"
#include "tag_parser.h"
#include "http_response.h"
#include "http_request.h"
#include "http_connection.h"
#include "crawler_viewer.h"

namespace CrawlerImpl
{
	using namespace HtmlParser;
	using namespace HttpLib;

    /**************************************************************************************************************
     *
     *   #1. Object of type Crawler saves in queue of internal links, only relative part of link.
     *   Likewise saves in indexed queue. The relative path URL must start with character slash "/".
     *
     **************************************************************************************************************
     *
     *   #2. Conversion functions relative addresses returns a new address, starting with the character slash "/".
     *
     **************************************************************************************************************
     *
     *   #3. Single slash - "/" in internal and indexed queues denotes the root of the site.
     *
     **************************************************************************************************************
     */


    class Crawler
    {
    public:

        Crawler();
        Crawler(const std::string& startAddr);
        Crawler(const std::string& startAddr, std::size_t timePause);

        void setStartAddress(const std::string& startAddr);
		void setSignature(const std::string& signature);
		void setViewer(const ViewHandler::CrawlerViewer* viewer);
		void setPause(std::size_t timePause);
		void setMaxDepth(std::size_t depth = 0);
		void setModeInfiniteCrawl(bool val);
		void setPathToSavePages(const std::string& path);

        bool readyForStart();
        void start();

        void clearInternalQueue();
        void clearExternalQueue();
        void clearIndexedInQueue();
        void clearIndexedExQueue();

		std::size_t getTimePause() const;
		std::size_t getMaxDepth() const;
		bool getModeInfiniteCrawl() const; 
		const std::string& getSignature() const;
		const std::string& getStartAddress() const;
		const std::string& getPathToSavePages() const;

	private:

		enum QueueType 
		{ 
			Internal, 
			External = 2, 
			IndexedIn = 4, 
			IndexedEx = 8 
		};

		enum class UrlType 
		{ 
			Internal, 
			External, 
			Default 
		};

	private:
		void crawlStart();
		void crawlResource();

		// the second argument is expected
		// relative address indicating the file or without it
		void addToQueue(const TagParser& parser, const Url& relativePath, UrlType urls = UrlType::Default);

		// finds link in indexed and internal queues
		bool existsInQueues(
			const HtmlParser::Url& url, 
			int whereToSearch = Internal
		);

		// add link into specified queue with check on uniqueness
		// if addition into internal queue then adds only relative path.
		// if addition into external queue then adds subdomain + host
		void addLinkToQueue(const HtmlParser::Url& url, int whereToSearch);

		// the flags you can pass through |. Old of C method.
		void clearQueue(int queueMark);

		// reset configurations with clear internal and indexedInternal queues
		void resetConfigurations();

		/** API for conversion true relative addresses **/

		// converts the relative path in accordance with the address where the received link
		// expects that second parameter is set by the website root
		std::string convertRelativeAddress(const HtmlParser::Url& relAddr, const HtmlParser::Url& where);

		// divides path by folders
		// expects path following [/]aaa/[bbb/[page.php[?var1=val&var2=val]]]
		// the last slash should be, otherwise will not correct the results
		std::vector<std::string> dividePath(const std::string& path);

	private:

		std::deque<std::string> m_indexedInternal;
		std::deque<std::string> m_indexedExternal;
		std::deque<std::string> m_internalLinks;
		std::deque<std::string> m_externalLinks;

		// pointer because objects of type HttpResponse
		// may create and edit only objects of type HttpConnection
		const HttpResponse* m_response;
		HttpRequest m_request;
		HttpConnection m_controller;

		// main website URL to crawl
		HtmlParser::Url m_host;

		// between requests
		std::size_t m_timePause;

		// full signature in User-agent
		std::string m_signature;

		// original signature of crawler
		std::string m_sign;
		std::size_t m_maxDepth;

		bool m_readyForStart;
		bool m_infiniteCrawl;

		std::string m_pathToSave;

		const ViewHandler::CrawlerViewer* m_viewerDevice = nullptr;

		// stores name of file which consists from words for generate phrases
		static const std::string s_storage_of_phrases;

		// name of file that stores errors generated by the class Apx::Url
		static const std::string s_reportUrl;

		// name of file that stores the addresses of sites passed
		static const std::string s_storage_urls;
    };

}