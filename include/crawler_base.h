#pragma once

#include "stdlibs.h"
#include "tag_parser.h"
#include "url.h"
#include "http_connection.h"

namespace CrawlerImpl
{

using namespace HtmlParser;
using namespace HttpLib;

class CrawlerBase
{
public:
	enum QueueType
	{
		Internal,
		External = 2,
		IndexedIn = 4,
		IndexedEx = 8
	};

	// the second argument is expected
	// relative address indicating the file or without it
	void addToCrawlQueue(const TagParser& parser, const Url& relativePath);

	// finds link in indexed and internal queues
	bool existsInQueues(const Url& url, int queueType = Internal);

	// add link into specified queue with check on uniqueness
	// if addition into internal queue then adds only relative path.
	// if addition into external queue then adds subdomain + host
	void storeLinkToQueue(const Url& url, int whereToSearch);

	// the flags you can pass through |. Old of C method.
	void clearQueue(int queueType);

	/** API for conversion true relative addresses **/

	// converts the relative path in accordance with the address where the received link
	// expects that second parameter is set by the website root
	std::string convertRelativeAddress(const Url& relAddr, const Url& where);

	// divides path by folders
	// expects path following [/]aaa/[bbb/[page.php[?var1=val&var2=val]]]
	// the last slash should be, otherwise will not correct the results
	std::vector<std::string> dividePath(const std::string& path);

	void printReport(const std::string& text);

protected:
	std::deque<std::string> m_indexedInternal;
	std::deque<std::string> m_indexedExternal;
	std::deque<std::string> m_internalLinks;
	std::deque<std::string> m_externalLinks;

	Url m_host;

	// stores name of file which consists from words for generate phrases
	static const std::string s_storage_of_phrases;

	// name of file that stores errors generated by the class Apx::Url
	static const std::string s_reportUrl;

	// name of file that stores the addresses of sites passed
	static const std::string s_storage_urls;
};

}